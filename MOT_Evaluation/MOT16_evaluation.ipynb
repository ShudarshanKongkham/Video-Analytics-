{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'MOT16', 'MOT16_evaluation.ipynb', 'track_evaluation.py', 'utils']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOT_Evaluation/MOT16/train/MOT16-11/gt/gt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import custom utility functions (ensure these are in your project)\n",
    "from utils.io import read_txt_to_struct, extract_valid_gt_data, print_metrics\n",
    "from utils.bbox import bbox_overlap\n",
    "from utils.measurements import clear_mot_hungarian, idmeasures\n",
    "\n",
    "def preprocessingDB(trackDB, gtDB, distractor_ids, iou_thres=0.5, minvis=0):\n",
    "    \"\"\"\n",
    "    Preprocess the computed trajectory data by matching computed boxes to ground truth.\n",
    "    Removes distractors and low visibility data in both trackDB and gtDB.\n",
    "\n",
    "    Parameters:\n",
    "    - trackDB: ndarray, computed trajectory data\n",
    "    - gtDB: ndarray, ground-truth data\n",
    "    - distractor_ids: ndarray, IDs of distractor objects in the sequence\n",
    "    - iou_thres: float, bounding box overlap threshold\n",
    "    - minvis: float, minimum visibility of ground truth boxes\n",
    "\n",
    "    Returns:\n",
    "    - trackDB: ndarray, preprocessed tracking data\n",
    "    - gtDB: ndarray, preprocessed ground truth data\n",
    "    \"\"\"\n",
    "    track_frames = np.unique(trackDB[:, 0])\n",
    "    gt_frames = np.unique(gtDB[:, 0])\n",
    "    nframes = min(len(track_frames), len(gt_frames))\n",
    "    res_keep = np.ones((trackDB.shape[0], ), dtype=float)\n",
    "\n",
    "    for frame_num in range(1, nframes + 1):\n",
    "        # Get indices of detections in the current frame\n",
    "        res_in_frame_idx = np.where(trackDB[:, 0] == frame_num)[0]\n",
    "        res_in_frame_data = trackDB[res_in_frame_idx, :]\n",
    "        gt_in_frame_idx = np.where(gtDB[:, 0] == frame_num)[0]\n",
    "        gt_in_frame_data = gtDB[gt_in_frame_idx, :]\n",
    "\n",
    "        res_num = res_in_frame_data.shape[0]\n",
    "        gt_num = gt_in_frame_data.shape[0]\n",
    "        overlaps = np.zeros((res_num, gt_num), dtype=float)\n",
    "\n",
    "        # Compute overlaps between detections and ground truth boxes\n",
    "        for gid in range(gt_num):\n",
    "            overlaps[:, gid] = bbox_overlap(\n",
    "                res_in_frame_data[:, 2:6], gt_in_frame_data[gid, 2:6])\n",
    "\n",
    "        # Perform assignment using the Hungarian algorithm\n",
    "        matched_indices = linear_sum_assignment(1 - overlaps)\n",
    "        for res_idx, gt_idx in zip(*matched_indices):\n",
    "            # Discard pairs with overlap lower than threshold\n",
    "            if overlaps[res_idx, gt_idx] < iou_thres:\n",
    "                continue\n",
    "\n",
    "            # Discard result box if matched to a distractor or low visibility object\n",
    "            if (gt_in_frame_data[gt_idx, 1] in distractor_ids or\n",
    "                gt_in_frame_data[gt_idx, 8] < minvis):\n",
    "                res_keep[res_in_frame_idx[res_idx]] = 0\n",
    "\n",
    "        # Check for duplicate IDs in the same frame\n",
    "        frame_id_pairs = res_in_frame_data[:, :2]\n",
    "        uniq_frame_id_pairs = np.unique(frame_id_pairs, axis=0)\n",
    "        has_duplicates = uniq_frame_id_pairs.shape[0] < frame_id_pairs.shape[0]\n",
    "        assert not has_duplicates, f'Duplicate ID in frame {frame_num}.'\n",
    "\n",
    "    # Keep only the valid detections\n",
    "    keep_idx = np.where(res_keep == 1)[0]\n",
    "    print(f'[TRACK PREPROCESSING]: Remaining {len(keep_idx)}/{len(res_keep)} computed boxes after removing distractors and low visibility boxes.')\n",
    "    trackDB = trackDB[keep_idx, :]\n",
    "\n",
    "    # Preprocess ground truth data\n",
    "    valid_gt_idx = np.array([\n",
    "        i for i in range(gtDB.shape[0])\n",
    "        if gtDB[i, 1] not in distractor_ids and gtDB[i, 8] >= minvis\n",
    "    ])\n",
    "    print(f'[GT PREPROCESSING]: Remaining {len(valid_gt_idx)}/{gtDB.shape[0]} ground truth boxes after removing distractors and low visibility boxes.')\n",
    "    gtDB = gtDB[valid_gt_idx, :]\n",
    "\n",
    "    return trackDB, gtDB\n",
    "\n",
    "def evaluate_sequence(trackDB, gtDB, distractor_ids, iou_thres=0.5, minvis=0):\n",
    "    \"\"\"\n",
    "    Evaluate a single sequence by computing tracking metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - trackDB: ndarray, tracking result data\n",
    "    - gtDB: ndarray, ground-truth data\n",
    "    - distractor_ids: ndarray, IDs of distractor objects\n",
    "    - iou_thres: float, bounding box overlap threshold\n",
    "    - minvis: float, minimum tolerable visibility\n",
    "\n",
    "    Returns:\n",
    "    - metrics: list, computed evaluation metrics\n",
    "    - extra_info: EasyDict, additional information from evaluation\n",
    "    \"\"\"\n",
    "    trackDB, gtDB = preprocessingDB(trackDB, gtDB, distractor_ids, iou_thres, minvis)\n",
    "    mme, c, fp, g, missed, d, M, allfps = clear_mot_hungarian(trackDB, gtDB, iou_thres)\n",
    "\n",
    "    gt_frames = np.unique(gtDB[:, 0])\n",
    "    gt_ids = np.unique(gtDB[:, 1])\n",
    "    st_ids = np.unique(trackDB[:, 1])\n",
    "    f_gt = len(gt_frames)\n",
    "    n_gt = len(gt_ids)\n",
    "    n_st = len(st_ids)\n",
    "\n",
    "    FN = sum(missed)\n",
    "    FP = sum(fp)\n",
    "    IDS = sum(mme)\n",
    "    MOTP = (sum(sum(d)) / sum(c)) * 100  # Multiple Object Tracking Precision\n",
    "    MOTAL = (1 - (FP + FN + np.log10(IDS + 1)) / sum(g)) * 100\n",
    "    MOTA = (1 - (FP + FN + IDS) / sum(g)) * 100  # Multiple Object Tracking Accuracy\n",
    "    recall = sum(c) / sum(g) * 100\n",
    "    precision = sum(c) / (FP + sum(c)) * 100\n",
    "    FAR = FP / f_gt  # False Alarm Rate\n",
    "\n",
    "    # Compute Mostly Tracked, Partially Tracked, Mostly Lost\n",
    "    MT_stats = np.zeros(n_gt, dtype=float)\n",
    "    for i in range(n_gt):\n",
    "        gt_id = gt_ids[i]\n",
    "        gt_indices = np.where(gtDB[:, 1] == gt_id)[0]\n",
    "        gt_length = len(gt_indices)\n",
    "        gt_frames_tmp = gtDB[gt_indices, 0].astype(int)\n",
    "        st_length = sum(1 if i in M[int(f - 1)].keys() else 0 for f in gt_frames_tmp)\n",
    "        ratio = float(st_length) / gt_length\n",
    "\n",
    "        if ratio >= 0.8:\n",
    "            MT_stats[i] = 3  # Mostly Tracked\n",
    "        elif ratio < 0.2:\n",
    "            MT_stats[i] = 1  # Mostly Lost\n",
    "        else:\n",
    "            MT_stats[i] = 2  # Partially Tracked\n",
    "\n",
    "    ML = np.sum(MT_stats == 1)\n",
    "    PT = np.sum(MT_stats == 2)\n",
    "    MT = np.sum(MT_stats == 3)\n",
    "\n",
    "    # Compute Fragments\n",
    "    fr = np.zeros(n_gt, dtype=int)\n",
    "    M_arr = np.zeros((f_gt, n_gt), dtype=int)\n",
    "    for i in range(f_gt):\n",
    "        for gid in M[i].keys():\n",
    "            M_arr[i, gid] = M[i][gid] + 1\n",
    "    for i in range(n_gt):\n",
    "        occurrences = np.where(M_arr[:, i] > 0)[0]\n",
    "        discontinuities = np.where(np.diff(occurrences) != 1)[0]\n",
    "        fr[i] = len(discontinuities)\n",
    "    FRA = np.sum(fr)\n",
    "\n",
    "    # Compute ID metrics\n",
    "    idmetrics = idmeasures(gtDB, trackDB, iou_thres)\n",
    "\n",
    "    metrics = [\n",
    "        idmetrics.IDF1, idmetrics.IDP, idmetrics.IDR, recall,\n",
    "        precision, FAR, n_gt, MT, PT, ML, FP, FN, IDS, FRA,\n",
    "        MOTA, MOTP, MOTAL\n",
    "    ]\n",
    "\n",
    "    extra_info = edict()\n",
    "    extra_info.mme = IDS\n",
    "    extra_info.c = sum(c)\n",
    "    extra_info.fp = FP\n",
    "    extra_info.g = sum(g)\n",
    "    extra_info.missed = FN\n",
    "    extra_info.d = d\n",
    "    extra_info.f_gt = f_gt\n",
    "    extra_info.n_gt = n_gt\n",
    "    extra_info.n_st = n_st\n",
    "    extra_info.ML = ML\n",
    "    extra_info.PT = PT\n",
    "    extra_info.MT = MT\n",
    "    extra_info.FRA = FRA\n",
    "    extra_info.idmetrics = idmetrics\n",
    "    extra_info.metrics = metrics  # Store metrics for plotting\n",
    "    return metrics, extra_info\n",
    "\n",
    "def evaluate_benchmark(all_info):\n",
    "    \"\"\"\n",
    "    Evaluate the entire benchmark by summarizing all metrics across sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - all_info: list of EasyDict, additional information from each sequence evaluation\n",
    "\n",
    "    Returns:\n",
    "    - metrics: list, summarized evaluation metrics\n",
    "    \"\"\"\n",
    "    f_gt = sum(info.f_gt for info in all_info)\n",
    "    n_gt = sum(info.n_gt for info in all_info)\n",
    "    n_st = sum(info.n_st for info in all_info)\n",
    "    c = sum(info.c for info in all_info)\n",
    "    g = sum(info.g for info in all_info)\n",
    "    fp = sum(info.fp for info in all_info)\n",
    "    missed = sum(info.missed for info in all_info)\n",
    "    ids = sum(info.mme for info in all_info)\n",
    "    MT = sum(info.MT for info in all_info)\n",
    "    PT = sum(info.PT for info in all_info)\n",
    "    ML = sum(info.ML for info in all_info)\n",
    "    FRA = sum(info.FRA for info in all_info)\n",
    "    overlap_sum = sum(sum(sum(info.d)) for info in all_info)\n",
    "    idmetrics_list = [info.idmetrics for info in all_info]\n",
    "\n",
    "    # Summarize ID metrics\n",
    "    IDTP = sum(m.IDTP for m in idmetrics_list)\n",
    "    IDFP = sum(m.IDFP for m in idmetrics_list)\n",
    "    IDFN = sum(m.IDFN for m in idmetrics_list)\n",
    "    nbox_gt = sum(m.nbox_gt for m in idmetrics_list)\n",
    "    nbox_st = sum(m.nbox_st for m in idmetrics_list)\n",
    "\n",
    "    IDP = IDTP / (IDTP + IDFP) * 100 if (IDTP + IDFP) > 0 else 0\n",
    "    IDR = IDTP / (IDTP + IDFN) * 100 if (IDTP + IDFN) > 0 else 0\n",
    "    IDF1 = 2 * IDTP / (nbox_gt + nbox_st) * 100 if (nbox_gt + nbox_st) > 0 else 0\n",
    "    FAR = fp / f_gt if f_gt > 0 else 0\n",
    "    MOTP = (overlap_sum / c) * 100 if c > 0 else 0\n",
    "    MOTAL = (1 - (fp + missed + np.log10(ids + 1)) / g) * 100 if g > 0 else 0\n",
    "    MOTA = (1 - (fp + missed + ids) / g) * 100 if g > 0 else 0\n",
    "    recall = c / g * 100 if g > 0 else 0\n",
    "    precision = c / (fp + c) * 100 if (fp + c) > 0 else 0\n",
    "\n",
    "    metrics = [\n",
    "        IDF1, IDP, IDR, recall, precision, FAR, n_gt,\n",
    "        MT, PT, ML, fp, missed, ids, FRA, MOTA, MOTP, MOTAL\n",
    "    ]\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_tracking(sequences, base_dir):\n",
    "    \"\"\"\n",
    "    Evaluate tracking results against ground truth data for given sequences.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences: list of str, names of sequences to evaluate\n",
    "    - base_dir: str, base directory containing all sequence folders\n",
    "    \"\"\"\n",
    "    all_info = []\n",
    "    for seqname in sequences:\n",
    "        print(f'\\nEvaluating sequence: {seqname}')\n",
    "\n",
    "        # Paths to res.txt and gt.txt based on your folder structure\n",
    "        seq_dir = os.path.join(base_dir, seqname)\n",
    "        track_res = os.path.join(seq_dir, 'yolov9', 'res.txt')\n",
    "        gt_file = os.path.join(seq_dir, 'gt', 'gt.txt')\n",
    "\n",
    "        # Check if files exist\n",
    "        assert os.path.exists(track_res), f'Tracking result {track_res} does not exist.'\n",
    "        assert os.path.exists(gt_file), f'Ground truth file {gt_file} does not exist.'\n",
    "\n",
    "        # Read tracking results and ground truth data\n",
    "        trackDB = read_txt_to_struct(track_res)\n",
    "        gtDB = read_txt_to_struct(gt_file)\n",
    "\n",
    "        # Preprocess ground truth data\n",
    "        gtDB, distractor_ids = extract_valid_gt_data(gtDB)\n",
    "\n",
    "        # Evaluate sequence\n",
    "        metrics, extra_info = evaluate_sequence(trackDB, gtDB, distractor_ids)\n",
    "        print_metrics(f'{seqname} Evaluation', metrics)\n",
    "        extra_info.seq_name = seqname  # Store sequence name for plotting\n",
    "        all_info.append(extra_info)\n",
    "\n",
    "    # Evaluate the entire benchmark\n",
    "    all_metrics = evaluate_benchmark(all_info)\n",
    "    print_metrics('Summary Evaluation', all_metrics)\n",
    "\n",
    "   # Generate evaluation plots\n",
    "    generate_plots(all_info, all_metrics)\n",
    "\n",
    "def generate_plots(all_info, summary_metrics):\n",
    "    \"\"\"\n",
    "    Generate plots of evaluation metrics for each sequence and the overall benchmark.\n",
    "\n",
    "    Parameters:\n",
    "    - all_info: list of EasyDict, additional information from each sequence evaluation\n",
    "    - summary_metrics: list, summarized evaluation metrics\n",
    "    \"\"\"\n",
    "    sequences = [info.seq_name for info in all_info]\n",
    "    MOTA_list = [info.metrics[14] for info in all_info]  # MOTA is at index 14\n",
    "    MOTP_list = [info.metrics[15] for info in all_info]  # MOTP is at index 15\n",
    "    IDF1_list = [info.metrics[0] for info in all_info]   # IDF1 is at index 0\n",
    "\n",
    "    # Plot MOTA for each sequence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(sequences, MOTA_list, color='skyblue')\n",
    "    plt.xlabel('Sequence')\n",
    "    plt.ylabel('MOTA (%)')\n",
    "    plt.title('Multiple Object Tracking Accuracy (MOTA) per Sequence')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot MOTP for each sequence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(sequences, MOTP_list, color='salmon')\n",
    "    plt.xlabel('Sequence')\n",
    "    plt.ylabel('MOTP (%)')\n",
    "    plt.title('Multiple Object Tracking Precision (MOTP) per Sequence')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot IDF1 Score for each sequence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(sequences, IDF1_list, color='lightgreen')\n",
    "    plt.xlabel('Sequence')\n",
    "    plt.ylabel('IDF1 Score (%)')\n",
    "    plt.title('IDF1 Score per Sequence')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Summary Metrics\n",
    "    metrics_names = ['IDF1', 'IDP', 'IDR', 'Recall', 'Precision', 'MOTA', 'MOTP']\n",
    "    summary_values = [\n",
    "        summary_metrics[0],  # IDF1\n",
    "        summary_metrics[1],  # IDP\n",
    "        summary_metrics[2],  # IDR\n",
    "        summary_metrics[3],  # Recall\n",
    "        summary_metrics[4],  # Precision\n",
    "        summary_metrics[14], # MOTA\n",
    "        summary_metrics[15]  # MOTP\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    bars = plt.bar(metrics_names, summary_values, color='orchid')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Value (%)')\n",
    "    plt.title('Summary of Evaluation Metrics')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    # Annotate bars with values\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                 f'{height:.2f}%', ha='center', va='bottom')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating sequence: MOT16-11\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tracking result MOT_Evaluation/MOT16/train\\MOT16-11\\yolov9\\res.txt does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24060\\1758345125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Run evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mevaluate_tracking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24060\\1027258262.py\u001b[0m in \u001b[0;36mevaluate_tracking\u001b[1;34m(sequences, base_dir)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;31m# Check if files exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'Tracking result {track_res} does not exist.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'Ground truth file {gt_file} does not exist.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Tracking result MOT_Evaluation/MOT16/train\\MOT16-11\\yolov9\\res.txt does not exist."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Define the sequences to evaluate\n",
    "    sequences = ['MOT16-11', 'MOT16-13']  # Replace with your sequence names\n",
    "\n",
    "    # Define the base directory containing all sequence folders\n",
    "    base_dir = 'MOT_Evaluation/MOT16/train'  # Replace with your base directory\n",
    "\n",
    "    # Run evaluation\n",
    "    evaluate_tracking(sequences, base_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
