{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-9-12 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "class YoloDetector():\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.load_model(model_name)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # self.device = 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if model_name:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_name, force_reload=True)\n",
    "        else:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        downscale_factor = 4 \n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        frame_resized = cv2.resize(frame, (width, height))\n",
    "        results = self.model(frame_resized)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        return labels, cord\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        return self.classes[int(x)]\n",
    "\n",
    "    def plot_boxes(self, results, frame, confidence=0.3):\n",
    "        labels, cord = results\n",
    "        detections = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "        for i in range(len(labels)):\n",
    "            row = cord[i]\n",
    "            if row[4] >= confidence:\n",
    "                x1, y1, x2, y2 = int(row[0] * width), int(row[1] * height), int(row[2] * width), int(row[3] * height)\n",
    "                \n",
    "                # if self.class_to_label(labels[i]) == 'person':\n",
    "                #     detections.append(([x1, y1, int(x2 - x1), int(y2 - y1)], row[4].item(), 'person'))\n",
    "                detections.append(([x1, y1, int(x2 - x1), int(y2 - y1)], row[4].item(), labels[i]))\n",
    "        \n",
    "        return frame, detections\n",
    "\n",
    "# Initialize YOLO detector\n",
    "detector = YoloDetector(model_name=None)\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "object_tracker = DeepSort(max_age=5,\n",
    "                n_init=2,\n",
    "                nms_max_overlap=1.0,\n",
    "                max_cosine_distance=0.7,\n",
    "                nn_budget=None,\n",
    "                embedder=\"mobilenet\",\n",
    "                half=True,\n",
    "                bgr=True,\n",
    "                embedder_gpu=True)\n",
    "\n",
    "# Open video capture\n",
    "# cap = cv2.VideoCapture(\"walking.mp4\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/data_/traffic_2.mp4\") \n",
    "\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Set up environment variable for compatibility\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    # Resize image using fx and fy for scaling\n",
    "    img = cv2.resize(img, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Perform detection\n",
    "    results = detector.score_frame(img)\n",
    "    img, detections = detector.plot_boxes(results, img, confidence=0.5)\n",
    "\n",
    "    # Update tracks\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "    # Draw boxes and IDs\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        \n",
    "        track_id = track.track_id\n",
    "        track_age = track.age\n",
    "        track_class = track.det_class\n",
    "        bbox = track.to_ltrb()  # Get bounding box in [left, top, right, bottom] format\n",
    "        \n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "        cv2.putText(img, f\"{track_class}\", (int(bbox[0]), int(bbox[1] + 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        cv2.putText(img, f\"ID: {track_id}\", (int(bbox[0]), int(bbox[1] - 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        cv2.putText(img, f\"AGE: {track_age}\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "    cv2.imshow('img', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper annotation and class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-9-12 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "class YoloDetector():\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.load_model(model_name)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if model_name:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_name, force_reload=True)\n",
    "        else:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        downscale_factor = 4 \n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        frame_resized = cv2.resize(frame, (width, height))\n",
    "        results = self.model(frame_resized)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        return labels, cord\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        return self.classes[int(x)]\n",
    "\n",
    "    def plot_boxes(self, results, frame, confidence=0.3):\n",
    "        labels, cord = results\n",
    "        detections = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "        for i in range(len(labels)):\n",
    "            row = cord[i]\n",
    "            if row[4] >= confidence:\n",
    "                x1, y1, x2, y2 = int(row[0] * width), int(row[1] * height), int(row[2] * width), int(row[3] * height)\n",
    "                \n",
    "                detections.append(([x1, y1, int(x2 - x1), int(y2 - y1)], row[4].item(), self.class_to_label(labels[i])))\n",
    "        \n",
    "        return frame, detections\n",
    "\n",
    "# Initialize YOLO detector\n",
    "detector = YoloDetector(model_name=None)\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "object_tracker = DeepSort(max_age=5,\n",
    "                n_init=2,\n",
    "                nms_max_overlap=1.0,\n",
    "                max_cosine_distance=0.7,\n",
    "                nn_budget=None,\n",
    "                embedder=\"mobilenet\",\n",
    "                half=True,\n",
    "                bgr=True,\n",
    "                embedder_gpu=True)\n",
    "\n",
    "# Specify the path to the img1 folder\n",
    "img_folder = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "\n",
    "# Set up environment variable for compatibility\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Process images from the folder instead of the webcam\n",
    "for img_path in img_paths:\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if img is None:\n",
    "        print(f\"Error reading image: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Perform detection\n",
    "    results = detector.score_frame(img)\n",
    "    img, detections = detector.plot_boxes(results, img, confidence=0.5)\n",
    "\n",
    "    # Update tracks\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "    # Draw boxes and IDs\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        track_age = track.age\n",
    "        track_class = track.det_class\n",
    "        bbox = track.to_ltrb()  # Get bounding box in [left, top, right, bottom] format\n",
    "\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "        cv2.putText(img, f\"{track_class}\", (int(bbox[0]), int(bbox[1] + 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        cv2.putText(img, f\"ID: {track_id}\", (int(bbox[0]), int(bbox[1] - 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        cv2.putText(img, f\"AGE: {track_age}\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "    cv2.imshow('YOLOv5 with DeepSORT', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only Person detection and save video in yolov5 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-9-12 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cuda\n",
      "Output video saved at: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13\\YOLOv5\\MOT16-13.avi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "class YoloDetector():\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.load_model(model_name)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if model_name:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_name, force_reload=True)\n",
    "        else:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        downscale_factor = 4 \n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        frame_resized = cv2.resize(frame, (width, height))\n",
    "        results = self.model(frame_resized)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        return labels, cord\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        return self.classes[int(x)]\n",
    "\n",
    "    def plot_boxes(self, results, frame, confidence=0.3):\n",
    "        labels, cord = results\n",
    "        detections = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "        for i in range(len(labels)):\n",
    "            row = cord[i]\n",
    "            if row[4] >= confidence and self.class_to_label(labels[i]) == 'person':  # Filter for 'person'\n",
    "                x1, y1, x2, y2 = int(row[0] * width), int(row[1] * height), int(row[2] * width), int(row[3] * height)\n",
    "                detections.append(([x1, y1, int(x2 - x1), int(y2 - y1)], row[4].item(), 'person'))  # Only 'person'\n",
    "\n",
    "        return frame, detections\n",
    "\n",
    "# Initialize YOLO detector\n",
    "detector = YoloDetector(model_name=None)\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "object_tracker = DeepSort(\n",
    "    max_age=5,\n",
    "    n_init=2,\n",
    "    nms_max_overlap=1.0,\n",
    "    max_cosine_distance=0.7,\n",
    "    nn_budget=None,\n",
    "    embedder=\"mobilenet\",\n",
    "    half=True,\n",
    "    bgr=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Specify the path to the img1 folder\n",
    "img_folder = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "\n",
    "# Create a 'YOLOv5' folder inside 'MOT16-13' for saving the output video\n",
    "output_folder = os.path.join(os.path.dirname(img_folder), \"YOLOv5\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Set output video path\n",
    "video_name = os.path.basename(os.path.dirname(img_folder)) + \".avi\"\n",
    "output_path = os.path.join(output_folder, video_name)\n",
    "\n",
    "# Load the first image to get frame dimensions\n",
    "sample_img = cv2.imread(img_paths[0])\n",
    "if sample_img is None:\n",
    "    print(f\"Error: Could not read {img_paths[0]}.\")\n",
    "    exit()\n",
    "\n",
    "height, width = sample_img.shape[:2]\n",
    "fps = 25  # Assuming 25 FPS\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Set up environment variable for compatibility\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Process each image\n",
    "for img_path in img_paths:\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Error reading image: {img_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Perform detection\n",
    "    results = detector.score_frame(img)\n",
    "    img, detections = detector.plot_boxes(results, img, confidence=0.5)\n",
    "\n",
    "    # Update tracks\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "    # Draw boxes and IDs\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_ltrb()  # Get bounding box [left, top, right, bottom]\n",
    "\n",
    "        # Draw bounding box and labels neatly\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "        cv2.putText(img, f\"Person\", (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.putText(img, f\"ID: {track_id}\", (int(bbox[0]), int(bbox[1] + 20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(img)\n",
    "\n",
    "    # Show the annotated frame\n",
    "    cv2.imshow('YOLOv5 with DeepSORT', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Press 'ESC' to exit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Output video saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store MOT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-9-12 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cuda\n",
      "Output video saved at: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13\\YOLOv5\\MOT16-13.avi\n",
      "Tracking results saved at: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13\\YOLOv5\\res.txt\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "class YoloDetector():\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.load_model(model_name)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if model_name:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_name, force_reload=True)\n",
    "        else:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        downscale_factor = 4 \n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        frame_resized = cv2.resize(frame, (width, height))\n",
    "        results = self.model(frame_resized)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        return labels, cord\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        return self.classes[int(x)]\n",
    "\n",
    "    def plot_boxes(self, results, frame, confidence=0.3):\n",
    "        labels, cord = results\n",
    "        detections = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "        for i in range(len(labels)):\n",
    "            row = cord[i]\n",
    "            if row[4] >= confidence and self.class_to_label(labels[i]) == 'person':  # Filter for 'person'\n",
    "                x1, y1, x2, y2 = int(row[0] * width), int(row[1] * height), int(row[2] * width), int(row[3] * height)\n",
    "                detections.append(([x1, y1, int(x2 - x1), int(y2 - y1)], row[4].item(), 'person'))  # Only 'person'\n",
    "\n",
    "        return frame, detections\n",
    "\n",
    "# Initialize YOLO detector\n",
    "detector = YoloDetector(model_name=None)\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "object_tracker = DeepSort(\n",
    "    max_age=5,\n",
    "    n_init=2,\n",
    "    nms_max_overlap=1.0,\n",
    "    max_cosine_distance=0.7,\n",
    "    nn_budget=None,\n",
    "    embedder=\"mobilenet\",\n",
    "    half=True,\n",
    "    bgr=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "# Specify the path to the img1 folder\n",
    "img_folder = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "\n",
    "# Create a 'YOLOv5' folder inside 'MOT16-13' for saving the output video\n",
    "output_folder = os.path.join(os.path.dirname(img_folder), \"YOLOv5\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Set output video path\n",
    "video_name = os.path.basename(os.path.dirname(img_folder)) + \".avi\"\n",
    "output_path = os.path.join(output_folder, video_name)\n",
    "\n",
    "# Initialize the res.txt file for storing tracking results\n",
    "res_file_path = os.path.join(output_folder, \"res.txt\")\n",
    "res_file = open(res_file_path, \"w\")\n",
    "\n",
    "# Load the first image to get frame dimensions\n",
    "sample_img = cv2.imread(img_paths[0])\n",
    "if sample_img is None:\n",
    "    print(f\"Error: Could not read {img_paths[0]}.\")\n",
    "    exit()\n",
    "\n",
    "height, width = sample_img.shape[:2]\n",
    "fps = 25  # Assuming 25 FPS\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Set up environment variable for compatibility\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Process each image\n",
    "for frame_id, img_path in enumerate(img_paths, start=1):\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Error reading image: {img_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Perform detection\n",
    "    results = detector.score_frame(img)\n",
    "    img, detections = detector.plot_boxes(results, img, confidence=0.5)\n",
    "\n",
    "    # Update tracks\n",
    "    tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "    # Draw boxes and IDs, and write to res.txt\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        bbox = track.to_ltrb()  # Get bounding box [left, top, right, bottom]\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "\n",
    "        # Write to res.txt in MOT16 format: <frame>,<id>,<x>,<y>,<w>,<h>,<confidence>,<class>,<visibility>\n",
    "        res_file.write(f\"{frame_id},{track_id},{x1},{y1},{x2 - x1},{y2 - y1},1.0,1,1\\n\")\n",
    "\n",
    "        # Draw bounding box and labels neatly\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(img, f\"Person\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.putText(img, f\"ID: {track_id}\", (x1, y1 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(img)\n",
    "\n",
    "    # Show the annotated frame\n",
    "    cv2.imshow('YOLOv5 with DeepSORT', img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Press 'ESC' to exit\n",
    "        break\n",
    "\n",
    "# Cleanup resources\n",
    "out.release()\n",
    "res_file.close()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Output video saved at: {output_path}\")\n",
    "print(f\"Tracking results saved at: {res_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lenovo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-9-12 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cuda\n",
      "Output video saved at: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13\\YOLOv5\\MOT16-13.avi\n",
      "Tracking results saved at: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13\\YOLOv5\\res.txt\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "class YoloDetector():\n",
    "    def __init__(self, model_name):\n",
    "        self.model = self.load_model(model_name)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Using Device: \", self.device)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if model_name:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_name, force_reload=True)\n",
    "        else:\n",
    "            model = torch.hub.load('ultralytics/yolov5', 'yolov5l', pretrained=True)\n",
    "        return model\n",
    "\n",
    "    def score_frame(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        downscale_factor = 4 \n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        frame_resized = cv2.resize(frame, (width, height))\n",
    "        results = self.model(frame_resized)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "        return labels, cord\n",
    "\n",
    "    def class_to_label(self, x):\n",
    "        return self.classes[int(x)]\n",
    "\n",
    "    def plot_boxes(self, results, frame, confidence=0.3):\n",
    "        labels, cord = results\n",
    "        detections = []\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "        for i in range(len(labels)):\n",
    "            row = cord[i]\n",
    "            if row[4] >= confidence and self.class_to_label(labels[i]) == 'person':  # Filter for 'person'\n",
    "                x1, y1, x2, y2 = int(row[0] * width), int(row[1] * height), int(row[2] * width), int(row[3] * height)\n",
    "                detections.append(([x1, y1, int(x2 - x1), int(y2 - y1)], row[4].item(), 'person'))  # Only 'person'\n",
    "\n",
    "        return frame, detections\n",
    "\n",
    "def process_images_from_folder(device, img_folder):\n",
    "    \"\"\"Process images from a folder and save results as video and res.txt.\"\"\"\n",
    "    detector = YoloDetector(model_name=None)\n",
    "\n",
    "    # Initialize DeepSORT tracker\n",
    "    object_tracker = DeepSort(\n",
    "        max_age=5,\n",
    "        n_init=2,\n",
    "        nms_max_overlap=1.0,\n",
    "        max_cosine_distance=0.7,\n",
    "        nn_budget=None,\n",
    "        embedder=\"mobilenet\",\n",
    "        half=True,\n",
    "        bgr=True,\n",
    "        embedder_gpu=True\n",
    "    )\n",
    "\n",
    "    # Prepare output paths\n",
    "    output_folder = os.path.join(os.path.dirname(img_folder), \"YOLOv5\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    video_name = os.path.basename(os.path.dirname(img_folder)) + \".avi\"\n",
    "    output_path = os.path.join(output_folder, video_name)\n",
    "    res_file_path = os.path.join(output_folder, \"res.txt\")\n",
    "\n",
    "    # Initialize video writer and res.txt\n",
    "    img_files = sorted(os.listdir(img_folder))\n",
    "    img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "    if not img_paths:\n",
    "        print(f\"No images found in {img_folder}\")\n",
    "        return\n",
    "\n",
    "    sample_img = cv2.imread(img_paths[0])\n",
    "    if sample_img is None:\n",
    "        print(f\"Error: Could not read {img_paths[0]}.\")\n",
    "        return\n",
    "\n",
    "    height, width = sample_img.shape[:2]\n",
    "    fps = 25  # Assuming 25 FPS\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    res_file = open(res_file_path, \"w\")\n",
    "\n",
    "    # Process each image\n",
    "    for frame_id, img_path in enumerate(img_paths, start=1):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Error reading image: {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        results = detector.score_frame(img)\n",
    "        img, detections = detector.plot_boxes(results, img, confidence=0.5)\n",
    "        tracks = object_tracker.update_tracks(detections, frame=img)\n",
    "\n",
    "        # Draw boxes, IDs, and write to res.txt\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "\n",
    "            track_id = track.track_id\n",
    "            bbox = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            res_file.write(f\"{frame_id},{track_id},{x1},{y1},{x2 - x1},{y2 - y1},1.0,1,1\\n\")\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            cv2.putText(img, \"Person\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.putText(img, f\"ID: {track_id}\", (x1, y1 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        totalTime = time.perf_counter() - start\n",
    "        fps = 1 / totalTime\n",
    "        cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "        out.write(img)\n",
    "        cv2.imshow('YOLOv5 with DeepSORT', img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    out.release()\n",
    "    res_file.close()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Output video saved at: {output_path}\")\n",
    "    print(f\"Tracking results saved at: {res_file_path}\")\n",
    "\n",
    "# Run the function for multiple sequences\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = 0  # Use GPU (0) or CPU ('cpu')\n",
    "    MOT_sequences = [\"MOT16-02\", \"MOT16-04\", \"MOT16-05\", \"MOT16-09\", \"MOT16-10\", \"MOT16-11\", \"MOT16-13\"]\n",
    "\n",
    "    # for MOT_folder in MOT_sequences:\n",
    "        # print(f\"Generating for {MOT_folder}...\")\n",
    "    img_folder = f\"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "    process_images_from_folder( device, img_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
