{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Width: 1920, Expected Height: 1080\n",
      "Actual Width: 1920, Actual Height: 1080\n",
      "Image dimensions match the seqinfo.ini specifications.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import configparser\n",
    "\n",
    "def locate_img1_folder(mot_path):\n",
    "    \"\"\"Locate the img1 folder inside the given MOT sequence path.\"\"\"\n",
    "    img1_path = os.path.join(mot_path, \"img1\")\n",
    "    if not os.path.exists(img1_path):\n",
    "        print(f\"Error: 'img1' folder not found in {mot_path}.\")\n",
    "        return None\n",
    "    return img1_path\n",
    "\n",
    "def get_seqinfo_dimensions(mot_path):\n",
    "    \"\"\"Parse seqinfo.ini to get the expected height and width of images.\"\"\"\n",
    "    seqinfo_path = os.path.join(mot_path, \"seqinfo.ini\")\n",
    "    if not os.path.exists(seqinfo_path):\n",
    "        print(f\"Error: 'seqinfo.ini' not found in {mot_path}.\")\n",
    "        return None\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(seqinfo_path)\n",
    "\n",
    "    try:\n",
    "        width = int(config[\"Sequence\"][\"imWidth\"])\n",
    "        height = int(config[\"Sequence\"][\"imHeight\"])\n",
    "        return width, height\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing key in seqinfo.ini: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_image_shape(mot_path):\n",
    "    \"\"\"Locate img1 folder, read image shapes, and compare with seqinfo dimensions.\"\"\"\n",
    "    # Locate img1 folder\n",
    "    img1_path = locate_img1_folder(mot_path)\n",
    "    if not img1_path:\n",
    "        return\n",
    "\n",
    "    # Get expected dimensions from seqinfo.ini\n",
    "    seqinfo_dims = get_seqinfo_dimensions(mot_path)\n",
    "    if not seqinfo_dims:\n",
    "        return\n",
    "\n",
    "    expected_width, expected_height = seqinfo_dims\n",
    "    print(f\"Expected Width: {expected_width}, Expected Height: {expected_height}\")\n",
    "\n",
    "    # Check the shape of the first image in img1\n",
    "    img_files = sorted(os.listdir(img1_path))\n",
    "    if not img_files:\n",
    "        print(f\"No images found in {img1_path}.\")\n",
    "        return\n",
    "\n",
    "    first_img_path = os.path.join(img1_path, img_files[0])\n",
    "    img = cv2.imread(first_img_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read {first_img_path}.\")\n",
    "        return\n",
    "\n",
    "    actual_height, actual_width = img.shape[:2]\n",
    "    print(f\"Actual Width: {actual_width}, Actual Height: {actual_height}\")\n",
    "\n",
    "    # Compare dimensions\n",
    "    if (actual_width, actual_height) == (expected_width, expected_height):\n",
    "        print(\"Image dimensions match the seqinfo.ini specifications.\")\n",
    "    else:\n",
    "        print(\"Warning: Image dimensions do NOT match the seqinfo.ini specifications.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    mot_path = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13\"\n",
    "    compare_image_shape(mot_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO  2024-4-17 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 604 layers, 50880768 parameters, 0 gradients, 237.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Resolution: Width=960, Height=540, FPS=25.0\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "Original Frame Size (Before Inference): (540, 960, 3)\n",
      "Original Frame Size: Width=960, Height=540\n",
      "Padded Frame Size: Width=960, Height=544\n",
      "Inference Input Size: torch.Size([1, 3, 544, 960])\n",
      "End of video or error reading frame.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import check_img_size, non_max_suppression, scale_boxes\n",
    "from utils.plots import Annotator, colors\n",
    "from utils.torch_utils import select_device\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "def load_model(weights, device):\n",
    "    \"\"\"Load the YOLO model with specified weights and device.\"\"\"\n",
    "    device = select_device(device)  # Select GPU or CPU\n",
    "    model = DetectMultiBackend(weights, device=device, fp16=True)  # Enable FP16 for GPU if supported\n",
    "    return model, model.names\n",
    "\n",
    "def resize_and_pad(image, stride=32):\n",
    "    \"\"\"Resize and pad the image to be compatible with the model's stride.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    print(f\"Original Frame Size: Width={w}, Height={h}\")  # Log original size\n",
    "\n",
    "    new_h = (h + stride - 1) // stride * stride\n",
    "    new_w = (w + stride - 1) // stride * stride\n",
    "\n",
    "    print(f\"Padded Frame Size: Width={new_w}, Height={new_h}\")  # Log padded size\n",
    "\n",
    "    padded_image = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "    padded_image[:h, :w, :] = image\n",
    "    return padded_image\n",
    "\n",
    "def inference(image, model, names, deepsort, line_thickness=2):\n",
    "    \"\"\"Perform inference and return annotated image and detections.\"\"\"\n",
    "    start_time = time.time()  # Track inference time for FPS calculation\n",
    "\n",
    "    # Resize and pad image to be stride-compatible\n",
    "    padded_image = resize_and_pad(image, stride=model.stride)\n",
    "\n",
    "    # Prepare image for inference\n",
    "    img = padded_image[..., ::-1].transpose(2, 0, 1)  # BGR to RGB, 3xHxW\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(model.device).float() / 255.0\n",
    "    if model.fp16:  # Use half precision if available\n",
    "        img = img.half()\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    print(f\"Inference Input Size: {img.shape}\")  # Log input size to the model\n",
    "\n",
    "    # Run inference\n",
    "    pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.45, max_det=100)\n",
    "\n",
    "    # Initialize annotator\n",
    "    im0 = image.copy()\n",
    "    annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "\n",
    "    # Process detections\n",
    "    detections = []\n",
    "    if len(pred[0]):\n",
    "        det = pred[0]\n",
    "        det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            cls_name = names[int(cls)]  # Class name from dictionary\n",
    "            detections.append(([x1, y1, x2 - x1, y2 - y1], conf.item(), cls_name))\n",
    "\n",
    "    # Update tracker\n",
    "    tracks = deepsort.update_tracks(detections, frame=im0)\n",
    "\n",
    "    # Annotate tracked objects\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 0:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        track_cls = track.det_class\n",
    "\n",
    "        # Reverse dictionary lookup to find the class index\n",
    "        cls_index = next((k for k, v in names.items() if v == track_cls), 0)\n",
    "\n",
    "        bbox = track.to_ltrb()  # left, top, right, bottom\n",
    "\n",
    "        # Assign a unique color to each class using the correct index\n",
    "        cls_color = colors(cls_index, True)\n",
    "\n",
    "        # Clean and readable annotations\n",
    "        label = f'ID: {track_id} | {track_cls}'\n",
    "        annotator.box_label(bbox, label, color=cls_color)\n",
    "\n",
    "    # Calculate FPS\n",
    "    fps = 1 / (time.time() - start_time)\n",
    "    cv2.putText(im0, f'FPS: {fps:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    return annotator.result()\n",
    "\n",
    "\n",
    "def run(weights, device, output_path='output.avi'):\n",
    "    \"\"\"Run real-time object detection and tracking using video input.\"\"\"\n",
    "    model, names = load_model(weights, device)\n",
    "    cap = cv2.VideoCapture(\"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/data/MOT16-13/MOT16-13-raw.webm\")\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Initialize video writer to save the output\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Use the input video's FPS\n",
    "    print(f\"Video Resolution: Width={width}, Height={height}, FPS={fps}\")  # Log video resolution\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize DeepSORT tracker\n",
    "    deepsort = DeepSort(\n",
    "        max_age=30, n_init=3, nms_max_overlap=1.0,\n",
    "        max_cosine_distance=0.7, nn_budget=None,\n",
    "        embedder_gpu=True, half=True\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or error reading frame.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Original Frame Size (Before Inference): {frame.shape}\")  # Log original frame size\n",
    "\n",
    "        # Perform inference and display the result\n",
    "        annotated_frame = inference(frame, model, names, deepsort)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow('YOLOv9 with DeepSORT', annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weights = 'yolov9-c.pt'  # Path to model weights\n",
    "    device = 0  # Use GPU (0) or CPU ('cpu')\n",
    "    run(weights, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO  2024-4-17 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 604 layers, 50880768 parameters, 0 gradients, 237.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000001.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000002.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000003.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000004.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000005.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000006.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000007.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000008.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000009.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000010.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000011.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000012.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000013.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000014.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000015.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000016.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000017.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000018.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000019.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000020.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000021.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000022.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000023.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000024.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000025.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000026.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000027.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000028.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000029.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000030.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000031.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000032.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000033.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000034.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000035.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000036.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000037.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000038.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000039.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000040.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000041.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000042.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000043.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000044.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000045.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000046.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000047.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000048.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000049.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000050.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000051.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000052.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000053.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000054.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000055.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000056.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000057.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000058.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000059.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000060.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000061.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000062.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000063.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000064.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000065.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000066.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000067.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000068.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000069.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000070.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000071.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000072.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000073.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000074.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000075.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000076.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000077.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000078.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000079.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000080.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000081.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000082.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000083.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000084.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000085.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000086.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000087.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000088.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000089.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000090.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000091.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000092.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000093.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000094.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000095.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000096.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000097.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000098.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000099.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000100.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000101.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000102.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000103.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000104.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000105.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000106.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000107.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000108.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000109.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000110.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000111.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000112.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000113.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000114.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000115.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000116.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000117.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000118.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000119.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000120.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000121.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000122.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000123.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000124.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000125.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000126.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000127.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000128.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000129.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000130.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000131.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000132.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000133.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000134.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000135.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000136.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000137.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000138.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000139.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000140.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000141.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000142.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000143.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000144.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000145.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000146.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000147.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000148.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000149.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000150.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000151.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000152.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000153.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000154.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000155.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000156.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000157.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000158.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000159.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000160.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000161.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000162.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000163.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000164.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000165.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000166.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000167.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000168.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000169.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000170.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000171.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000172.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000173.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000174.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000175.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000176.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000177.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000178.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000179.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000180.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000181.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000182.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000183.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000184.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000185.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000186.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000187.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000188.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000189.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000190.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000191.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000192.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000193.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000194.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000195.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000196.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000197.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000198.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000199.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000200.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000201.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000202.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000203.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000204.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000205.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000206.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000207.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000208.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000209.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000210.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000211.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000212.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000213.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000214.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000215.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000216.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000217.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000218.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000219.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000220.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000221.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000222.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000223.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000224.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000225.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000226.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000227.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000228.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000229.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000230.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000231.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000232.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000233.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000234.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000235.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000236.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000237.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000238.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000239.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000240.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000241.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000242.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000243.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000244.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000245.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000246.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000247.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000248.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000249.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000250.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000251.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000252.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000253.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000254.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000255.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000256.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000257.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000258.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000259.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000260.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000261.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000262.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000263.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000264.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000265.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000266.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000267.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000268.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000269.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000270.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000271.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000272.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000273.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000274.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000275.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000276.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000277.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000278.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000279.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000280.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000281.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000282.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000283.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000284.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000285.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000286.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000287.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000288.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000289.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000290.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000291.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000292.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000293.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000294.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000295.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000296.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000297.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000298.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000299.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000300.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000301.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000302.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000303.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000304.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000305.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000306.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000307.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000308.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000309.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000310.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000311.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000312.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000313.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000314.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000315.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000316.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000317.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000318.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000319.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000320.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000321.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000322.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000323.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000324.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000325.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000326.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000327.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000328.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000329.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000330.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000331.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000332.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000333.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000334.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000335.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000336.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000337.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000338.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000339.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000340.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000341.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000342.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000343.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000344.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000345.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000346.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000347.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000348.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000349.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000350.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000351.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000352.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000353.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000354.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000355.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000356.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000357.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000358.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000359.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000360.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000361.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000362.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000363.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000364.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000365.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000366.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000367.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000368.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000369.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000370.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000371.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000372.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000373.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000374.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000375.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000376.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000377.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000378.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000379.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000380.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000381.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000382.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000383.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000384.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000385.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000386.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000387.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000388.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000389.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000390.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000391.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000392.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000393.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000394.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000395.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000396.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000397.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000398.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000399.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000400.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000401.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000402.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000403.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000404.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000405.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000406.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000407.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000408.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000409.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000410.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000411.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000412.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000413.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000414.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000415.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000416.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000417.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000418.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000419.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000420.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000421.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000422.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000423.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000424.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000425.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000426.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000427.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000428.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000429.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000430.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000431.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000432.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000433.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000434.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000435.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000436.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000437.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000438.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000439.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000440.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000441.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000442.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000443.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000444.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000445.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000446.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000447.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000448.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000449.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000450.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000451.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000452.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000453.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000454.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000455.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000456.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000457.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000458.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000459.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000460.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000461.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000462.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000463.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000464.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000465.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000466.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000467.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000468.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000469.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000470.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000471.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000472.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000473.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000474.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000475.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000476.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000477.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000478.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000479.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000480.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000481.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000482.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000483.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000484.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000485.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000486.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000487.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000488.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000489.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000490.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000491.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000492.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000493.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000494.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000495.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000496.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000497.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000498.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000499.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000500.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000501.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000502.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000503.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000504.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000505.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000506.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000507.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000508.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000509.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000510.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000511.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000512.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000513.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000514.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000515.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000516.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000517.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000518.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000519.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000520.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000521.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000522.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000523.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000524.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000525.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000526.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000527.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000528.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000529.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000530.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000531.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000532.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000533.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000534.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000535.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000536.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000537.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000538.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000539.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000540.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000541.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000542.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000543.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000544.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000545.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000546.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000547.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000548.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000549.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000550.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000551.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000552.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000553.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000554.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000555.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000556.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000557.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000558.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000559.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000560.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000561.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000562.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000563.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000564.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000565.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000566.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000567.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000568.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000569.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000570.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000571.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000572.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000573.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000574.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000575.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000576.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000577.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000578.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000579.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000580.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000581.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000582.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000583.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000584.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000585.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000586.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000587.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000588.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000589.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000590.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000591.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000592.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000593.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000594.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000595.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000596.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000597.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000598.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000599.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000600.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000601.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000602.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000603.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000604.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000605.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000606.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000607.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000608.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000609.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000610.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000611.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000612.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000613.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000614.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000615.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000616.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000617.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000618.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000619.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000620.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000621.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000622.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000623.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000624.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000625.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000626.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000627.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000628.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000629.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000630.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000631.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000632.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000633.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000634.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000635.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000636.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000637.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000638.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000639.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000640.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000641.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000642.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000643.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000644.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000645.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000646.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000647.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000648.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000649.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000650.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000651.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000652.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000653.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000654.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000655.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000656.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000657.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000658.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000659.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000660.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000661.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000662.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000663.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000664.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000665.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000666.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000667.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000668.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000669.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000670.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000671.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000672.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000673.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000674.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000675.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000676.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000677.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000678.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000679.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000680.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000681.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000682.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000683.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000684.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000685.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000686.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000687.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000688.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000689.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000690.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000691.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000692.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000693.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000694.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000695.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000696.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000697.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000698.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000699.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000700.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000701.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000702.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000703.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000704.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000705.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000706.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000707.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000708.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000709.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000710.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000711.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000712.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000713.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000714.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000715.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000716.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000717.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000718.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000719.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000720.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000721.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000722.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000723.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000724.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000725.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000726.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000727.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000728.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000729.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000730.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000731.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000732.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000733.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000734.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000735.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000736.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000737.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000738.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000739.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000740.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000741.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000742.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000743.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000744.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000745.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000746.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000747.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000748.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000749.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000750.jpg\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import check_img_size, non_max_suppression, scale_boxes\n",
    "from utils.plots import Annotator, colors\n",
    "from utils.torch_utils import select_device\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "def load_model(weights, device):\n",
    "    \"\"\"Load the YOLO model with specified weights and device.\"\"\"\n",
    "    device = select_device(device)  # Select GPU or CPU\n",
    "    model = DetectMultiBackend(weights, device=device, fp16=True)  # Enable FP16 for GPU if supported\n",
    "    return model, model.names\n",
    "\n",
    "def resize_and_pad(image, stride=32):\n",
    "    \"\"\"Resize and pad the image to be compatible with the model's stride.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    new_h = (h + stride - 1) // stride * stride\n",
    "    new_w = (w + stride - 1) // stride * stride\n",
    "    padded_image = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "    padded_image[:h, :w, :] = image\n",
    "    return padded_image\n",
    "\n",
    "def inference(image, model, names, deepsort, line_thickness=2):\n",
    "    \"\"\"Perform inference and return annotated image and detections.\"\"\"\n",
    "    start_time = time.time()  # Track inference time for FPS calculation\n",
    "\n",
    "    # Resize and pad image to be stride-compatible\n",
    "    padded_image = resize_and_pad(image, stride=model.stride)\n",
    "\n",
    "    # Prepare image for inference\n",
    "    img = padded_image[..., ::-1].transpose(2, 0, 1)  # BGR to RGB, 3xHxW\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(model.device).float() / 255.0\n",
    "    if model.fp16:  # Use half precision if available\n",
    "        img = img.half()\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference\n",
    "    pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.45, max_det=100)\n",
    "\n",
    "    # Initialize annotator\n",
    "    im0 = image.copy()\n",
    "    annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "\n",
    "    # Process detections\n",
    "    detections = []\n",
    "    if len(pred[0]):\n",
    "        det = pred[0]\n",
    "        det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            cls_name = names[int(cls)]  # Class name from dictionary\n",
    "            detections.append(([x1, y1, x2 - x1, y2 - y1], conf.item(), cls_name))\n",
    "\n",
    "    # Update tracker\n",
    "    tracks = deepsort.update_tracks(detections, frame=im0)\n",
    "\n",
    "    # Annotate tracked objects\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 0:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        track_cls = track.det_class\n",
    "\n",
    "        # Reverse dictionary lookup to find the class index\n",
    "        cls_index = next((k for k, v in names.items() if v == track_cls), 0)\n",
    "\n",
    "        bbox = track.to_ltrb()  # left, top, right, bottom\n",
    "\n",
    "        # Assign a unique color to each class using the correct index\n",
    "        cls_color = colors(cls_index, True)\n",
    "\n",
    "        # Clean and readable annotations\n",
    "        label = f'ID: {track_id} | {track_cls}'\n",
    "        annotator.box_label(bbox, label, color=cls_color)\n",
    "\n",
    "    # Calculate FPS\n",
    "    fps = 1 / (time.time() - start_time)\n",
    "    cv2.putText(im0, f'FPS: {fps:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    return annotator.result()\n",
    "\n",
    "def process_images_from_folder(weights, device, img_folder, output_path='Frames_output.avi'):\n",
    "    \"\"\"Process images from a folder and save results to a video.\"\"\"\n",
    "    model, names = load_model(weights, device)\n",
    "\n",
    "    # Get image file paths\n",
    "    img_files = sorted(os.listdir(img_folder))\n",
    "    img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "\n",
    "    if len(img_paths) == 0:\n",
    "        print(f\"No images found in {img_folder}\")\n",
    "        return\n",
    "\n",
    "    # Load the first image to get frame dimensions\n",
    "    sample_img = cv2.imread(img_paths[0])\n",
    "    if sample_img is None:\n",
    "        print(f\"Error: Could not read {img_paths[0]}.\")\n",
    "        return\n",
    "\n",
    "    height, width = sample_img.shape[:2]\n",
    "    fps = 25  # Assuming 25 FPS for the output video\n",
    "\n",
    "    # Initialize video writer to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize DeepSORT tracker\n",
    "    deepsort = DeepSort(\n",
    "        max_age=30, n_init=3, nms_max_overlap=1.0,\n",
    "        max_cosine_distance=0.7, nn_budget=None,\n",
    "        embedder_gpu=True, half=True\n",
    "    )\n",
    "\n",
    "    # Process each image\n",
    "    for img_path in img_paths:\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"Error: Could not read {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {img_path}\")\n",
    "\n",
    "        # Perform inference and get annotated frame\n",
    "        annotated_frame = inference(frame, model, names, deepsort)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow('YOLOv9 with DeepSORT', annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weights = 'yolov9-c.pt'  # Path to model weights\n",
    "    device = 0  # Use GPU (0) or CPU ('cpu')\n",
    "    img_folder = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "    process_images_from_folder(weights, device, img_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform only person detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO  2024-4-17 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 604 layers, 50880768 parameters, 0 gradients, 237.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000001.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000002.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000003.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000004.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000005.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000006.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000007.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000008.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000009.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000010.jpg\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000011.jpg\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import check_img_size, non_max_suppression, scale_boxes\n",
    "from utils.plots import Annotator, colors\n",
    "from utils.torch_utils import select_device\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "def load_model(weights, device):\n",
    "    \"\"\"Load the YOLO model with specified weights and device.\"\"\"\n",
    "    device = select_device(device)  # Select GPU or CPU\n",
    "    model = DetectMultiBackend(weights, device=device, fp16=True)  # Enable FP16 for GPU if supported\n",
    "    return model, model.names\n",
    "\n",
    "def resize_and_pad(image, stride=32):\n",
    "    \"\"\"Resize and pad the image to be compatible with the model's stride.\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    new_h = (h + stride - 1) // stride * stride\n",
    "    new_w = (w + stride - 1) // stride * stride\n",
    "    padded_image = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "    padded_image[:h, :w, :] = image\n",
    "    return padded_image\n",
    "\n",
    "def inference(image, model, names, deepsort, line_thickness=2):\n",
    "    \"\"\"Perform inference and return annotated image with person detections only.\"\"\"\n",
    "    start_time = time.time()  # Track inference time for FPS calculation\n",
    "\n",
    "    # Resize and pad image to be stride-compatible\n",
    "    padded_image = resize_and_pad(image, stride=model.stride)\n",
    "\n",
    "    # Prepare image for inference\n",
    "    img = padded_image[..., ::-1].transpose(2, 0, 1)  # BGR to RGB, 3xHxW\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(model.device).float() / 255.0\n",
    "    if model.fp16:  # Use half precision if available\n",
    "        img = img.half()\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference\n",
    "    pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.45, max_det=100)\n",
    "\n",
    "    # Initialize annotator\n",
    "    im0 = image.copy()\n",
    "    annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "\n",
    "    # Process person-only detections\n",
    "    person_index = next((k for k, v in names.items() if v == \"person\"), None)\n",
    "\n",
    "    detections = []\n",
    "    if person_index is not None and len(pred[0]):\n",
    "        det = pred[0]\n",
    "        det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            if int(cls) == person_index:  # Filter for person detections only\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cls_name = names[int(cls)]  # Class name from dictionary\n",
    "                detections.append(([x1, y1, x2 - x1, y2 - y1], conf.item(), cls_name))\n",
    "\n",
    "    # Update tracker with person-only detections\n",
    "    tracks = deepsort.update_tracks(detections, frame=im0)\n",
    "\n",
    "    # Annotate tracked objects\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 0:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        track_cls = track.det_class\n",
    "\n",
    "        bbox = track.to_ltrb()  # left, top, right, bottom\n",
    "\n",
    "        # Assign a unique color to the person class\n",
    "        cls_color = colors(0, True)  # Assuming \"person\" is class index 0\n",
    "\n",
    "        # Clean and readable annotations\n",
    "        label = f'ID: {track_id} | {track_cls}'\n",
    "        annotator.box_label(bbox, label, color=cls_color)\n",
    "\n",
    "    # Calculate FPS\n",
    "    fps = 1 / (time.time() - start_time)\n",
    "    cv2.putText(im0, f'FPS: {fps:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    return annotator.result()\n",
    "\n",
    "def process_images_from_folder(weights, device, img_folder, output_path='Frames_person.avi'):\n",
    "    \"\"\"Process images from a folder and save results to a video.\"\"\"\n",
    "    model, names = load_model(weights, device)\n",
    "\n",
    "    # Get image file paths\n",
    "    img_files = sorted(os.listdir(img_folder))\n",
    "    img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "\n",
    "    if len(img_paths) == 0:\n",
    "        print(f\"No images found in {img_folder}\")\n",
    "        return\n",
    "\n",
    "    # Load the first image to get frame dimensions\n",
    "    sample_img = cv2.imread(img_paths[0])\n",
    "    if sample_img is None:\n",
    "        print(f\"Error: Could not read {img_paths[0]}.\")\n",
    "        return\n",
    "\n",
    "    height, width = sample_img.shape[:2]\n",
    "    fps = 25  # Assuming 25 FPS for the output video\n",
    "\n",
    "    # Initialize video writer to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize DeepSORT tracker\n",
    "    deepsort = DeepSort(\n",
    "        max_age=30, n_init=3, nms_max_overlap=1.0,\n",
    "        max_cosine_distance=0.7, nn_budget=None,\n",
    "        embedder_gpu=True, half=True\n",
    "    )\n",
    "\n",
    "    # Process each image\n",
    "    for img_path in img_paths:\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"Error: Could not read {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {img_path}\")\n",
    "\n",
    "        # Perform inference and get annotated frame\n",
    "        annotated_frame = inference(frame, model, names, deepsort)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow('YOLOv9 with DeepSORT', annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weights = 'yolov9-c.pt'  # Path to model weights\n",
    "    device = 0  # Use GPU (0) or CPU ('cpu')\n",
    "    img_folder = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "    process_images_from_folder(weights, device, img_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person DEtection locate image folder and save in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO  2024-4-17 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 604 layers, 50880768 parameters, 0 gradients, 237.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000001.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000002.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000003.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000004.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000005.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000006.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000007.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000008.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000009.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000010.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000011.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000012.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000013.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000014.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000015.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000016.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000017.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000018.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000019.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000020.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000021.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000022.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000023.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000024.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000025.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000026.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000027.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000028.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000029.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000030.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000031.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000032.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000033.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000034.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000035.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000036.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000037.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000038.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000039.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000040.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000041.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000042.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000043.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000044.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000045.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000046.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000047.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000048.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000049.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000050.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000051.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000052.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000053.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000054.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000055.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000056.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000057.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000058.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000059.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000060.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000061.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000062.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000063.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000064.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000065.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000066.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000067.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000068.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000069.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000070.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000071.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000072.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000073.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000074.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000075.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000076.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000077.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000078.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000079.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000080.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000081.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000082.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000083.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000084.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000085.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000086.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000087.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000088.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000089.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000090.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000091.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000092.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000093.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000094.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000095.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000096.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000097.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000098.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000099.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000100.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000101.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000102.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000103.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000104.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000105.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000106.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000107.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000108.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000109.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000110.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000111.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000112.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000113.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000114.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000115.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000116.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000117.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000118.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000119.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000120.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000121.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000122.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000123.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000124.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000125.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000126.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000127.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000128.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000129.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000130.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000131.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000132.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000133.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000134.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000135.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000136.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000137.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000138.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000139.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000140.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000141.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000142.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000143.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000144.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000145.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000146.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000147.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000148.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000149.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000150.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000151.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000152.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000153.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000154.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000155.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000156.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000157.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000158.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000159.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000160.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000161.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000162.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000163.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000164.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000165.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000166.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000167.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000168.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000169.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000170.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000171.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000172.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000173.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000174.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000175.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000176.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000177.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000178.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000179.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000180.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000181.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000182.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000183.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000184.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000185.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000186.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000187.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000188.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000189.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000190.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000191.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000192.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000193.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000194.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000195.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000196.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000197.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000198.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000199.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000200.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000201.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000202.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000203.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000204.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000205.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000206.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000207.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000208.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000209.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000210.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000211.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000212.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000213.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000214.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000215.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000216.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000217.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000218.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000219.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000220.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000221.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000222.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000223.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000224.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000225.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000226.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000227.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000228.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000229.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000230.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000231.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000232.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000233.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000234.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000235.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000236.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000237.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000238.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000239.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000240.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000241.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000242.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000243.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000244.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000245.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000246.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000247.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000248.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000249.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000250.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000251.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000252.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000253.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000254.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000255.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000256.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000257.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000258.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000259.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000260.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000261.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000262.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000263.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000264.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000265.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000266.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000267.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000268.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000269.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000270.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000271.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000272.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000273.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000274.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000275.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000276.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000277.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000278.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000279.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000280.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000281.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000282.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000283.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000284.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000285.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000286.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000287.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000288.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000289.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000290.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000291.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000292.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000293.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000294.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000295.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000296.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000297.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000298.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000299.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000300.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000301.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000302.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000303.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000304.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000305.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000306.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000307.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000308.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000309.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000310.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000311.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000312.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000313.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000314.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000315.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000316.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000317.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000318.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000319.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000320.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000321.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000322.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000323.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000324.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000325.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000326.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000327.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000328.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000329.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000330.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000331.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000332.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000333.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000334.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000335.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000336.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000337.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000338.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000339.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000340.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000341.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000342.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000343.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000344.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000345.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000346.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000347.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000348.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000349.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000350.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000351.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000352.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000353.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000354.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000355.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000356.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000357.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000358.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000359.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000360.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000361.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000362.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000363.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000364.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000365.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000366.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000367.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000368.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000369.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000370.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000371.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000372.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000373.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000374.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000375.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000376.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000377.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000378.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000379.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000380.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000381.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000382.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000383.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000384.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000385.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000386.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000387.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000388.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000389.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000390.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000391.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000392.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000393.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000394.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000395.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000396.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000397.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000398.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000399.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000400.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000401.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000402.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000403.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000404.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000405.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000406.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000407.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000408.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000409.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000410.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000411.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000412.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000413.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000414.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000415.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000416.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000417.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000418.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000419.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000420.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000421.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000422.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000423.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000424.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000425.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000426.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000427.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000428.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000429.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000430.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000431.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000432.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000433.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000434.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000435.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000436.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000437.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000438.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000439.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000440.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000441.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000442.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000443.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000444.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000445.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000446.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000447.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000448.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000449.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000450.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000451.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000452.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000453.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000454.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000455.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000456.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000457.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000458.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000459.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000460.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000461.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000462.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000463.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000464.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000465.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000466.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000467.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000468.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000469.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000470.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000471.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000472.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000473.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000474.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000475.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000476.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000477.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000478.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000479.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000480.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000481.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000482.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000483.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000484.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000485.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000486.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000487.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000488.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000489.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000490.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000491.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000492.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000493.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000494.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000495.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000496.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000497.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000498.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000499.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000500.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000501.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000502.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000503.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000504.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000505.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000506.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000507.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000508.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000509.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000510.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000511.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000512.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000513.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000514.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000515.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000516.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000517.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000518.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000519.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000520.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000521.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000522.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000523.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000524.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000525.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000526.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000527.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000528.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000529.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000530.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000531.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000532.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000533.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000534.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000535.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000536.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000537.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000538.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000539.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000540.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000541.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000542.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000543.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000544.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000545.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000546.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000547.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000548.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000549.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000550.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000551.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000552.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000553.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000554.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000555.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000556.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000557.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000558.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000559.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000560.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000561.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000562.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000563.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000564.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000565.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000566.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000567.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000568.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000569.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000570.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000571.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000572.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000573.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000574.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000575.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000576.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000577.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000578.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000579.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000580.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000581.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000582.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000583.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000584.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000585.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000586.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000587.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000588.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000589.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000590.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000591.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000592.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000593.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000594.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000595.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000596.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000597.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000598.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000599.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000600.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000601.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000602.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000603.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000604.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000605.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000606.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000607.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000608.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000609.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000610.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000611.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000612.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000613.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000614.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000615.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000616.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000617.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000618.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000619.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000620.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000621.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000622.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000623.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000624.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000625.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000626.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000627.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000628.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000629.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000630.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000631.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000632.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000633.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000634.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000635.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000636.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000637.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000638.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000639.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000640.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000641.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000642.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000643.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000644.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000645.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000646.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000647.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000648.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000649.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000650.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000651.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000652.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000653.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000654.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000655.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000656.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000657.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000658.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000659.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000660.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000661.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000662.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000663.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000664.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000665.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000666.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000667.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000668.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000669.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000670.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000671.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000672.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000673.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000674.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000675.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000676.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000677.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000678.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000679.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000680.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000681.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000682.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000683.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000684.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000685.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000686.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000687.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000688.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000689.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000690.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000691.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000692.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000693.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000694.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000695.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000696.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000697.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000698.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000699.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000700.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000701.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000702.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000703.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000704.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000705.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000706.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000707.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000708.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000709.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000710.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000711.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000712.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000713.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000714.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000715.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000716.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000717.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000718.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000719.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000720.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000721.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000722.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000723.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000724.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000725.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000726.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000727.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000728.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000729.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000730.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000731.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000732.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000733.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000734.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000735.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000736.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000737.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000738.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000739.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000740.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000741.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000742.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000743.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000744.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000745.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000746.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000747.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000748.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000749.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000750.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import check_img_size, non_max_suppression, scale_boxes\n",
    "from utils.plots import Annotator, colors\n",
    "from utils.torch_utils import select_device\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "def load_model(weights, device):\n",
    "    \"\"\"Load the YOLO model with specified weights and device.\"\"\"\n",
    "    device = select_device(device)  # Select GPU or CPU\n",
    "    model = DetectMultiBackend(weights, device=device, fp16=True)  # Enable FP16 for GPU if supported\n",
    "    return model, model.names\n",
    "\n",
    "def resize_and_pad(image, stride=32):\n",
    "    \"\"\"Resize and pad the image to be compatible with the model's stride.\"\"\"\n",
    "    print(\"Before Padding\", image.shape)\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    new_h = (h + stride - 1) // stride * stride\n",
    "    new_w = (w + stride - 1) // stride * stride\n",
    "    padded_image = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "    padded_image[:h, :w, :] = image\n",
    "    print(\"After Padding\",padded_image.shape)\n",
    "    return padded_image\n",
    "\n",
    "def inference(image, model, names, deepsort, line_thickness=2):\n",
    "    \"\"\"Perform inference and return annotated image with person detections only.\"\"\"\n",
    "    start_time = time.time()  # Track inference time for FPS calculation\n",
    "\n",
    "    # Resize and pad image to be stride-compatible\n",
    "    padded_image = resize_and_pad(image, stride=model.stride)\n",
    "\n",
    "    # Prepare image for inference\n",
    "    img = padded_image[..., ::-1].transpose(2, 0, 1)  # BGR to RGB, 3xHxW\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(model.device).float() / 255.0\n",
    "    if model.fp16:  # Use half precision if available\n",
    "        img = img.half()\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference\n",
    "    pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.45, max_det=100)\n",
    "\n",
    "    # Initialize annotator\n",
    "    im0 = image.copy()\n",
    "    annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "\n",
    "    # Process person-only detections\n",
    "    person_index = next((k for k, v in names.items() if v == \"person\"), None)\n",
    "\n",
    "    detections = []\n",
    "    if person_index is not None and len(pred[0]):\n",
    "        det = pred[0]\n",
    "        det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            if int(cls) == person_index:  # Filter for person detections only\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cls_name = names[int(cls)]  # Class name from dictionary\n",
    "                detections.append(([x1, y1, x2 - x1, y2 - y1], conf.item(), cls_name))\n",
    "\n",
    "    # Update tracker with person-only detections\n",
    "    tracks = deepsort.update_tracks(detections, frame=im0)\n",
    "\n",
    "    # Annotate tracked objects\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 0:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        track_cls = track.det_class\n",
    "\n",
    "        bbox = track.to_ltrb()  # left, top, right, bottom\n",
    "\n",
    "        # Assign a unique color to the person class\n",
    "        cls_color = colors(0, True)  # Assuming \"person\" is class index 0\n",
    "\n",
    "        # Clean and readable annotations\n",
    "        label = f'ID: {track_id} | {track_cls}'\n",
    "        annotator.box_label(bbox, label, color=cls_color)\n",
    "\n",
    "    # Calculate FPS\n",
    "    fps = 1 / (time.time() - start_time)\n",
    "    cv2.putText(im0, f'FPS: {fps:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    return annotator.result()\n",
    "\n",
    "def process_images_from_folder(weights, device, img_folder):\n",
    "    \"\"\"Process images from a folder and save results to a video.\"\"\"\n",
    "    model, names = load_model(weights, device)\n",
    "\n",
    "    # Get the parent directory to create the 'yolov9' folder inside it\n",
    "    parent_dir = os.path.dirname(img_folder)\n",
    "    output_dir = os.path.join(parent_dir, \"yolov9\")\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create 'yolov9' folder if not exists\n",
    "\n",
    "    # Set the output video name based on the parent folder name\n",
    "    video_name = os.path.basename(parent_dir) + \".avi\"\n",
    "    output_path = os.path.join(output_dir, video_name)\n",
    "\n",
    "    # Get image file paths\n",
    "    img_files = sorted(os.listdir(img_folder))\n",
    "    img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "\n",
    "    if len(img_paths) == 0:\n",
    "        print(f\"No images found in {img_folder}\")\n",
    "        return\n",
    "\n",
    "    # Load the first image to get frame dimensions\n",
    "    sample_img = cv2.imread(img_paths[0])\n",
    "    if sample_img is None:\n",
    "        print(f\"Error: Could not read {img_paths[0]}.\")\n",
    "        return\n",
    "\n",
    "    height, width = sample_img.shape[:2]\n",
    "    fps = 25  # Assuming 25 FPS for the output video\n",
    "\n",
    "    # Initialize video writer to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize DeepSORT tracker\n",
    "    deepsort = DeepSort(\n",
    "        max_age=30, n_init=3, nms_max_overlap=1.0,\n",
    "        max_cosine_distance=0.7, nn_budget=None,\n",
    "        embedder_gpu=True, half=True\n",
    "    )\n",
    "\n",
    "    # Process each image\n",
    "    for img_path in img_paths:\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"Error: Could not read {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {img_path}\")\n",
    "\n",
    "        # Perform inference and get annotated frame\n",
    "        annotated_frame = inference(frame, model, names, deepsort)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow('YOLOv9 with DeepSORT', annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weights = 'yolov9-c.pt'  # Path to model weights\n",
    "    device = 0  # Use GPU (0) or CPU ('cpu')\n",
    "    img_folder = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "    process_images_from_folder(weights, device, img_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the MOT evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO  2024-4-17 Python-3.11.9 torch-2.2.2+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 604 layers, 50880768 parameters, 0 gradients, 237.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frame 1: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000001.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 2: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000002.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 3: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000003.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 4: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000004.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 5: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000005.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 6: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000006.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 7: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000007.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 8: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000008.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 9: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000009.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 10: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000010.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 11: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000011.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 12: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000012.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 13: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000013.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 14: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000014.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 15: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000015.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 16: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000016.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 17: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000017.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 18: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000018.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 19: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000019.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 20: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000020.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 21: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000021.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 22: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000022.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 23: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000023.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 24: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000024.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 25: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000025.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 26: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000026.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 27: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000027.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 28: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000028.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 29: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000029.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 30: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000030.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 31: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000031.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 32: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000032.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 33: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000033.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 34: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000034.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 35: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000035.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 36: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000036.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 37: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000037.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 38: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000038.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 39: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000039.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 40: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000040.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 41: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000041.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 42: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000042.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 43: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000043.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 44: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000044.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 45: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000045.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 46: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000046.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 47: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000047.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 48: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000048.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 49: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000049.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 50: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000050.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 51: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000051.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 52: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000052.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 53: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000053.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 54: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000054.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 55: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000055.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 56: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000056.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 57: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000057.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 58: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000058.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 59: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000059.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 60: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000060.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 61: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000061.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 62: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000062.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 63: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000063.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 64: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000064.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 65: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000065.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 66: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000066.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 67: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000067.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 68: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000068.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 69: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000069.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 70: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000070.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 71: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000071.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 72: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000072.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 73: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000073.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 74: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000074.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 75: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000075.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 76: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000076.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 77: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000077.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 78: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000078.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 79: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000079.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 80: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000080.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 81: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000081.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 82: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000082.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 83: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000083.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 84: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000084.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 85: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000085.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 86: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000086.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 87: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000087.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 88: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000088.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 89: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000089.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 90: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000090.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 91: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000091.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 92: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000092.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 93: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000093.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 94: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000094.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 95: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000095.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 96: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000096.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 97: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000097.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 98: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000098.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 99: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000099.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 100: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000100.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 101: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000101.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 102: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000102.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 103: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000103.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 104: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000104.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 105: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000105.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 106: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000106.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 107: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000107.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 108: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000108.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 109: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000109.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 110: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000110.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 111: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000111.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 112: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000112.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 113: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000113.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 114: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000114.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 115: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000115.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 116: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000116.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 117: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000117.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 118: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000118.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 119: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000119.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 120: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000120.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 121: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000121.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 122: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000122.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 123: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000123.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 124: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000124.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 125: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000125.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 126: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000126.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 127: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000127.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 128: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000128.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 129: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000129.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 130: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000130.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 131: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000131.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 132: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000132.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 133: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000133.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 134: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000134.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 135: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000135.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 136: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000136.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 137: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000137.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 138: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000138.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 139: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000139.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 140: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000140.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 141: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000141.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 142: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000142.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 143: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000143.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 144: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000144.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 145: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000145.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 146: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000146.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 147: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000147.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 148: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000148.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 149: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000149.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 150: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000150.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 151: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000151.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 152: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000152.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 153: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000153.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 154: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000154.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 155: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000155.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 156: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000156.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 157: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000157.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 158: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000158.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 159: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000159.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 160: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000160.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 161: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000161.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 162: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000162.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 163: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000163.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 164: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000164.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 165: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000165.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 166: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000166.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 167: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000167.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 168: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000168.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 169: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000169.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 170: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000170.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 171: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000171.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 172: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000172.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 173: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000173.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 174: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000174.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 175: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000175.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 176: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000176.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 177: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000177.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 178: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000178.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 179: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000179.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 180: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000180.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 181: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000181.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 182: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000182.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 183: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000183.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 184: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000184.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 185: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000185.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 186: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000186.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 187: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000187.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 188: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000188.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 189: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000189.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 190: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000190.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 191: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000191.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 192: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000192.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 193: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000193.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 194: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000194.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 195: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000195.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 196: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000196.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 197: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000197.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 198: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000198.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 199: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000199.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 200: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000200.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 201: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000201.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 202: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000202.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 203: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000203.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 204: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000204.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 205: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000205.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 206: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000206.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 207: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000207.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 208: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000208.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 209: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000209.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 210: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000210.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 211: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000211.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 212: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000212.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 213: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000213.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 214: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000214.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 215: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000215.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 216: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000216.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 217: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000217.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 218: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000218.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 219: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000219.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 220: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000220.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 221: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000221.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 222: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000222.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 223: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000223.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 224: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000224.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 225: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000225.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 226: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000226.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 227: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000227.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 228: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000228.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 229: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000229.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 230: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000230.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 231: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000231.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 232: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000232.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 233: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000233.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 234: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000234.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 235: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000235.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 236: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000236.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 237: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000237.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 238: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000238.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 239: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000239.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 240: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000240.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 241: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000241.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 242: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000242.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 243: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000243.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 244: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000244.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 245: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000245.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 246: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000246.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 247: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000247.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 248: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000248.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 249: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000249.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 250: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000250.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 251: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000251.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 252: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000252.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 253: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000253.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 254: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000254.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 255: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000255.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 256: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000256.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 257: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000257.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 258: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000258.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 259: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000259.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 260: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000260.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 261: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000261.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 262: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000262.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 263: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000263.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 264: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000264.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 265: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000265.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 266: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000266.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 267: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000267.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 268: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000268.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 269: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000269.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 270: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000270.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 271: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000271.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 272: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000272.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 273: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000273.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 274: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000274.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 275: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000275.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 276: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000276.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 277: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000277.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 278: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000278.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 279: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000279.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 280: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000280.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 281: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000281.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 282: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000282.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 283: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000283.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 284: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000284.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 285: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000285.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 286: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000286.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 287: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000287.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 288: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000288.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 289: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000289.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 290: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000290.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 291: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000291.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 292: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000292.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 293: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000293.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 294: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000294.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 295: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000295.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 296: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000296.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 297: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000297.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 298: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000298.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 299: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000299.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 300: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000300.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 301: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000301.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 302: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000302.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 303: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000303.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 304: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000304.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 305: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000305.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 306: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000306.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 307: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000307.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 308: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000308.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 309: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000309.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 310: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000310.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 311: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000311.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 312: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000312.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 313: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000313.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 314: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000314.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 315: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000315.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 316: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000316.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 317: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000317.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 318: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000318.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 319: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000319.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 320: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000320.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 321: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000321.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 322: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000322.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 323: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000323.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 324: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000324.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 325: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000325.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 326: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000326.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 327: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000327.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 328: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000328.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 329: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000329.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 330: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000330.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 331: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000331.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 332: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000332.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 333: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000333.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 334: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000334.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 335: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000335.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 336: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000336.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 337: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000337.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 338: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000338.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 339: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000339.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 340: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000340.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 341: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000341.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 342: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000342.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 343: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000343.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 344: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000344.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 345: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000345.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 346: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000346.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 347: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000347.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 348: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000348.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 349: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000349.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 350: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000350.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 351: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000351.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 352: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000352.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 353: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000353.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 354: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000354.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 355: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000355.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 356: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000356.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 357: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000357.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 358: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000358.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 359: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000359.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 360: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000360.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 361: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000361.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 362: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000362.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 363: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000363.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 364: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000364.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 365: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000365.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 366: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000366.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 367: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000367.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 368: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000368.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 369: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000369.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 370: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000370.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 371: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000371.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 372: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000372.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 373: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000373.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 374: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000374.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 375: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000375.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 376: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000376.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 377: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000377.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 378: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000378.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 379: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000379.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 380: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000380.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 381: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000381.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 382: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000382.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 383: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000383.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 384: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000384.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 385: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000385.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 386: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000386.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 387: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000387.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 388: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000388.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 389: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000389.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 390: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000390.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 391: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000391.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 392: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000392.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 393: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000393.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 394: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000394.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 395: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000395.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 396: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000396.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 397: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000397.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 398: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000398.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 399: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000399.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 400: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000400.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 401: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000401.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 402: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000402.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 403: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000403.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 404: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000404.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 405: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000405.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 406: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000406.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 407: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000407.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 408: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000408.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 409: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000409.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 410: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000410.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 411: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000411.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 412: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000412.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 413: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000413.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 414: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000414.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 415: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000415.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 416: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000416.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 417: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000417.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 418: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000418.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 419: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000419.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 420: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000420.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 421: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000421.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 422: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000422.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 423: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000423.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 424: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000424.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 425: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000425.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 426: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000426.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 427: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000427.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 428: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000428.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 429: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000429.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 430: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000430.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 431: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000431.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 432: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000432.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 433: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000433.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 434: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000434.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 435: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000435.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 436: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000436.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 437: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000437.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 438: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000438.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 439: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000439.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 440: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000440.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 441: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000441.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 442: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000442.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 443: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000443.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 444: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000444.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 445: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000445.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 446: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000446.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 447: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000447.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 448: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000448.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 449: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000449.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 450: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000450.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 451: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000451.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 452: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000452.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 453: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000453.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 454: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000454.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 455: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000455.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 456: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000456.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 457: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000457.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 458: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000458.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 459: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000459.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 460: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000460.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 461: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000461.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 462: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000462.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 463: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000463.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 464: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000464.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 465: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000465.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 466: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000466.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 467: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000467.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 468: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000468.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 469: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000469.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 470: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000470.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 471: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000471.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 472: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000472.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 473: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000473.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 474: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000474.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 475: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000475.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 476: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000476.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 477: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000477.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 478: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000478.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 479: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000479.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 480: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000480.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 481: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000481.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 482: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000482.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 483: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000483.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 484: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000484.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 485: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000485.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 486: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000486.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 487: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000487.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 488: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000488.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 489: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000489.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 490: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000490.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 491: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000491.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 492: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000492.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 493: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000493.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 494: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000494.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 495: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000495.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 496: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000496.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 497: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000497.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 498: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000498.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 499: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000499.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 500: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000500.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 501: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000501.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 502: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000502.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 503: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000503.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 504: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000504.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 505: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000505.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 506: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000506.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 507: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000507.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 508: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000508.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 509: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000509.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 510: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000510.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 511: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000511.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 512: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000512.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 513: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000513.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 514: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000514.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 515: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000515.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 516: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000516.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 517: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000517.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 518: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000518.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 519: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000519.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 520: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000520.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 521: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000521.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 522: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000522.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 523: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000523.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 524: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000524.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 525: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000525.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 526: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000526.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 527: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000527.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 528: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000528.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 529: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000529.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 530: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000530.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 531: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000531.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 532: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000532.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 533: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000533.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 534: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000534.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 535: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000535.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 536: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000536.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 537: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000537.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 538: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000538.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 539: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000539.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 540: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000540.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 541: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000541.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 542: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000542.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 543: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000543.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 544: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000544.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 545: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000545.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 546: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000546.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 547: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000547.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 548: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000548.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 549: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000549.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 550: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000550.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 551: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000551.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 552: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000552.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 553: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000553.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 554: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000554.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 555: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000555.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 556: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000556.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 557: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000557.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 558: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000558.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 559: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000559.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 560: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000560.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 561: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000561.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 562: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000562.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 563: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000563.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 564: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000564.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 565: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000565.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 566: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000566.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 567: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000567.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 568: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000568.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 569: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000569.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 570: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000570.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 571: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000571.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 572: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000572.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 573: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000573.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 574: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000574.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 575: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000575.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 576: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000576.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 577: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000577.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 578: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000578.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 579: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000579.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 580: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000580.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 581: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000581.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 582: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000582.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 583: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000583.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 584: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000584.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 585: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000585.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 586: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000586.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 587: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000587.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 588: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000588.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 589: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000589.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 590: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000590.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 591: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000591.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 592: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000592.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 593: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000593.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 594: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000594.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 595: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000595.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 596: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000596.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 597: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000597.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 598: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000598.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 599: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000599.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 600: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000600.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 601: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000601.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 602: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000602.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 603: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000603.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 604: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000604.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 605: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000605.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 606: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000606.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 607: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000607.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 608: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000608.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 609: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000609.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 610: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000610.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 611: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000611.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 612: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000612.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 613: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000613.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 614: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000614.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 615: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000615.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 616: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000616.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 617: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000617.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 618: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000618.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 619: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000619.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 620: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000620.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 621: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000621.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 622: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000622.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 623: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000623.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 624: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000624.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 625: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000625.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 626: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000626.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 627: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000627.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 628: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000628.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 629: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000629.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 630: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000630.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 631: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000631.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 632: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000632.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 633: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000633.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 634: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000634.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 635: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000635.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 636: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000636.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 637: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000637.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 638: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000638.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 639: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000639.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 640: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000640.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 641: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000641.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 642: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000642.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 643: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000643.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 644: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000644.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 645: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000645.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 646: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000646.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 647: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000647.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 648: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000648.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 649: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000649.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 650: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000650.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 651: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000651.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 652: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000652.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 653: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000653.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 654: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000654.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 655: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000655.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 656: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000656.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 657: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000657.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 658: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000658.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 659: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000659.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 660: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000660.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 661: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000661.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 662: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000662.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 663: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000663.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 664: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000664.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 665: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000665.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 666: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000666.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 667: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000667.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 668: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000668.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 669: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000669.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 670: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000670.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 671: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000671.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 672: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000672.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 673: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000673.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 674: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000674.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 675: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000675.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 676: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000676.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 677: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000677.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 678: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000678.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 679: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000679.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 680: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000680.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 681: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000681.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 682: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000682.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 683: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000683.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 684: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000684.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 685: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000685.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 686: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000686.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 687: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000687.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 688: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000688.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 689: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000689.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 690: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000690.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 691: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000691.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 692: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000692.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 693: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000693.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 694: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000694.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 695: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000695.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 696: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000696.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 697: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000697.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 698: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000698.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 699: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000699.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 700: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000700.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 701: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000701.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 702: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000702.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 703: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000703.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 704: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000704.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 705: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000705.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 706: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000706.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 707: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000707.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 708: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000708.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 709: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000709.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 710: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000710.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 711: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000711.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 712: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000712.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 713: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000713.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 714: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000714.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 715: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000715.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 716: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000716.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 717: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000717.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 718: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000718.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 719: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000719.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 720: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000720.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 721: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000721.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 722: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000722.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 723: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000723.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 724: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000724.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 725: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000725.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 726: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000726.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 727: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000727.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 728: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000728.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 729: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000729.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 730: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000730.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 731: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000731.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 732: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000732.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 733: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000733.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 734: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000734.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 735: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000735.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 736: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000736.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 737: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000737.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 738: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000738.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 739: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000739.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 740: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000740.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 741: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000741.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 742: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000742.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 743: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000743.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 744: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000744.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 745: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000745.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 746: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000746.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 747: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000747.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 748: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000748.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 749: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000749.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Processing Frame 750: G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\\000750.jpg\n",
      "Before Padding (1080, 1920, 3)\n",
      "After Padding (1088, 1920, 3)\n",
      "Tracking results saved to G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13\\yolov9\\res.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.general import check_img_size, non_max_suppression, scale_boxes\n",
    "from utils.plots import Annotator, colors\n",
    "from utils.torch_utils import select_device\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "def load_model(weights, device):\n",
    "    \"\"\"Load the YOLO model with specified weights and device.\"\"\"\n",
    "    device = select_device(device)  # Select GPU or CPU\n",
    "    model = DetectMultiBackend(weights, device=device, fp16=True)  # Enable FP16 for GPU if supported\n",
    "    return model, model.names\n",
    "\n",
    "def resize_and_pad(image, stride=32):\n",
    "    \"\"\"Resize and pad the image to be compatible with the model's stride.\"\"\"\n",
    "    print(\"Before Padding\", image.shape)\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    new_h = (h + stride - 1) // stride * stride\n",
    "    new_w = (w + stride - 1) // stride * stride\n",
    "    padded_image = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "    padded_image[:h, :w, :] = image\n",
    "    print(\"After Padding\",padded_image.shape)\n",
    "    return padded_image\n",
    "\n",
    "def inference(image, model, names, deepsort, line_thickness=2):\n",
    "    \"\"\"Perform inference and return annotated image with tracking data.\"\"\"\n",
    "    start_time = time.time()  # Track inference time for FPS calculation\n",
    "\n",
    "    # Resize and pad image to be stride-compatible\n",
    "    padded_image = resize_and_pad(image, stride=model.stride)\n",
    "\n",
    "    # Prepare image for inference\n",
    "    img = padded_image[..., ::-1].transpose(2, 0, 1)  # BGR to RGB, 3xHxW\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(model.device).float() / 255.0\n",
    "    if model.fp16:  # Use half precision if available\n",
    "        img = img.half()\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference\n",
    "    pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.45, max_det=100)\n",
    "\n",
    "    # Initialize annotator\n",
    "    im0 = image.copy()\n",
    "    annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "\n",
    "    # Process person-only detections\n",
    "    person_index = next((k for k, v in names.items() if v == \"person\"), None)\n",
    "\n",
    "    detections = []\n",
    "    if person_index is not None and len(pred[0]):\n",
    "        det = pred[0]\n",
    "        det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            if int(cls) == person_index:  # Filter for person detections only\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cls_name = names[int(cls)]  # Class name from dictionary\n",
    "                detections.append(([x1, y1, x2 - x1, y2 - y1], conf.item(), cls_name))\n",
    "\n",
    "    # Update tracker with person-only detections\n",
    "    tracks = deepsort.update_tracks(detections, frame=im0)\n",
    "\n",
    "    # Initialize list to store tracking data for this frame\n",
    "    tracking_data = []\n",
    "\n",
    "    # Annotate tracked objects and collect tracking data\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 0:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        track_cls = track.det_class\n",
    "\n",
    "        bbox = track.to_ltrb()  # left, top, right, bottom\n",
    "\n",
    "        # Assign a unique color to the person class\n",
    "        cls_color = colors(0, True)  # Assuming \"person\" is class index 0\n",
    "\n",
    "        # Clean and readable annotations\n",
    "        label = f'ID: {track_id} | {track_cls}'\n",
    "        annotator.box_label(bbox, label, color=cls_color)\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        bbox_left, bbox_top, bbox_right, bbox_bottom = bbox\n",
    "        bbox_width = bbox_right - bbox_left\n",
    "        bbox_height = bbox_bottom - bbox_top\n",
    "\n",
    "        # Get detection confidence from the track\n",
    "        confidence = track.det_conf  # Ensure DeepSORT returns this attribute\n",
    "\n",
    "        # Append data to tracking_data\n",
    "        tracking_data.append([track_id, bbox_left, bbox_top, bbox_width, bbox_height, confidence])\n",
    "\n",
    "    # Calculate FPS\n",
    "    fps = 1 / (time.time() - start_time)\n",
    "    cv2.putText(im0, f'FPS: {fps:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    return annotator.result(), tracking_data\n",
    "\n",
    "\n",
    "\n",
    "def process_images_from_folder(weights, device, img_folder):\n",
    "    \"\"\"Process images from a folder, save results to a video, and create res.txt.\"\"\"\n",
    "    model, names = load_model(weights, device)\n",
    "\n",
    "    # Get the parent directory to create the 'yolov9' folder inside it\n",
    "    parent_dir = os.path.dirname(img_folder)\n",
    "    output_dir = os.path.join(parent_dir, \"yolov9\")\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create 'yolov9' folder if not exists\n",
    "\n",
    "    # Set the output video name based on the parent folder name\n",
    "    video_name = os.path.basename(parent_dir) + \".avi\"\n",
    "    output_path = os.path.join(output_dir, video_name)\n",
    "\n",
    "    # Get image file paths\n",
    "    img_files = sorted(os.listdir(img_folder))\n",
    "    img_paths = [os.path.join(img_folder, img_file) for img_file in img_files]\n",
    "\n",
    "    if len(img_paths) == 0:\n",
    "        print(f\"No images found in {img_folder}\")\n",
    "        return\n",
    "\n",
    "    # Load the first image to get frame dimensions\n",
    "    sample_img = cv2.imread(img_paths[0])\n",
    "    if sample_img is None:\n",
    "        print(f\"Error: Could not read {img_paths[0]}.\")\n",
    "        return\n",
    "\n",
    "    height, width = sample_img.shape[:2]\n",
    "    fps = 25  # Assuming 25 FPS for the output video\n",
    "\n",
    "    # Initialize video writer to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for .avi format\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize DeepSORT tracker\n",
    "    deepsort = DeepSort(\n",
    "        max_age=30, n_init=3, nms_max_overlap=1.0,\n",
    "        max_cosine_distance=0.7, nn_budget=None,\n",
    "        embedder_gpu=True, half=True\n",
    "    )\n",
    "\n",
    "    # Initialize tracking results list\n",
    "    tracking_results = []\n",
    "\n",
    "    # Process each image\n",
    "    for frame_number, img_path in enumerate(img_paths, start=1):\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"Error: Could not read {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing Frame {frame_number}: {img_path}\")\n",
    "\n",
    "        # Perform inference and get annotated frame and tracking data\n",
    "        annotated_frame, tracking_data = inference(frame, model, names, deepsort)\n",
    "\n",
    "        # Collect tracking results\n",
    "        for data in tracking_data:\n",
    "            track_id, bbox_left, bbox_top, bbox_width, bbox_height, confidence = data\n",
    "            # Prepare the line in required format\n",
    "            res_line = [\n",
    "                frame_number,  # Frame number starting from 1\n",
    "                track_id,\n",
    "                bbox_left,\n",
    "                bbox_top,\n",
    "                bbox_width,\n",
    "                bbox_height,\n",
    "                confidence,\n",
    "                -1, -1, -1  # Placeholders for world coordinates\n",
    "            ]\n",
    "            tracking_results.append(res_line)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow('YOLOv9 with DeepSORT', annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Write tracking results to res.txt\n",
    "    res_txt_path = os.path.join(output_dir, 'res.txt')\n",
    "    with open(res_txt_path, 'w') as f:\n",
    "        for res_line in tracking_results:\n",
    "            line_str = ','.join(map(str, res_line))\n",
    "            f.write(line_str + '\\n')\n",
    "\n",
    "    print(f\"Tracking results saved to {res_txt_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weights = 'yolov9-c.pt'  # Path to model weights\n",
    "    device = 0  # Use GPU (0) or CPU ('cpu')\n",
    "    img_folder = \"G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/MOT_Evaluation/MOT16/train/MOT16-13/img1\"\n",
    "    process_images_from_folder(weights, device, img_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
