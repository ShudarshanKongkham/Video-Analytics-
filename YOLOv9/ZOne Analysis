import time
import numpy as np
import torch
import cv2
from models.common import DetectMultiBackend
from utils.general import non_max_suppression
from utils.plots import Annotator
from utils.torch_utils import select_device
from deep_sort_realtime.deepsort_tracker import DeepSort

# Global Variables
zones = []  # Store zones with center, entry, and exit lines
drawing = False  # Drawing state
start_point = None  # Start point during line drawing

object_paths = {}  # Track object paths
zone_counts = {}  # Track entry/exit counts for each zone

### 1. Load YOLO Model
def load_model(weights, device):
    """Load the YOLO model."""
    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, fp16=True)
    return model, model.names

### 2. Generate Unique Color
def generate_unique_color(track_id):
    np.random.seed(int(track_id) % 1000)  # Modulo to avoid overflows
    return tuple(np.random.randint(0, 255, 3).tolist())

### 3. Mouse Callback for Zone Definition
def mouse_callback(event, x, y, flags, param):
    global drawing, start_point, zones

    if event == cv2.EVENT_LBUTTONDOWN:  # Start drawing on left-click
        drawing = True
        start_point = (x, y)

    elif event == cv2.EVENT_LBUTTONUP and drawing:  # Finish drawing on release
        drawing = False
        center_line = (start_point, (x, y))
        entry_line = ((start_point[0], start_point[1] - 20), (x, y - 20))
        exit_line = ((start_point[0], start_point[1] + 20), (x, y + 20))

        zones.append({"center": center_line, "entry": entry_line, "exit": exit_line})
        zone_counts[len(zones) - 1] = {"entered": 0, "exited": 0}

    elif event == cv2.EVENT_RBUTTONDOWN and zones:  # Undo last zone on right-click
        zones.pop()

### 4. Draw Zones Consistently on Frame
def draw_zones(frame):
    """Draw all defined zones (center, entry, exit lines)."""
    for i, zone in enumerate(zones):
        cv2.line(frame, zone["center"][0], zone["center"][1], (255, 255, 255), 2)
        cv2.line(frame, zone["entry"][0], zone["entry"][1], (0, 255, 0), 2)
        cv2.line(frame, zone["exit"][0], zone["exit"][1], (0, 0, 255), 2)
        cv2.putText(frame, f'Zone {i + 1}', zone["center"][0], cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

### 5. Analyze Zone Crossings
def analyze_crossing(track_id, path):
    """Analyze if the object entered/exited any zone."""
    if len(path) < 2:
        return

    prev_y, curr_y = path[-2][1], path[-1][1]

    for zone_id, zone in enumerate(zones):
        entry_y = zone["entry"][0][1]
        exit_y = zone["exit"][0][1]

        if prev_y < entry_y <= curr_y:
            zone_counts[zone_id]["entered"] += 1
        elif prev_y > exit_y >= curr_y:
            zone_counts[zone_id]["exited"] += 1

### 6. Perform Inference
def inference(frame, model, names, tracker, path_frame):
    """Perform inference and return annotated frames."""
    img = np.ascontiguousarray(frame[..., ::-1].transpose(2, 0, 1))
    img = torch.from_numpy(img).to(model.device).float() / 255.0

    if img.ndimension() == 3:
        img = img.unsqueeze(0)

    img = img.half() if model.fp16 else img

    with torch.no_grad():
        predictions = model(img)
        pred = non_max_suppression(predictions[0], conf_thres=0.5, iou_thres=0.45, max_det=100)

    annotator = Annotator(frame, line_width=2, example=str(names))

    detections = [
        ([int(xyxy[0]), int(xyxy[1]), int(xyxy[2] - xyxy[0]), int(xyxy[3] - xyxy[1])],
         conf.item(), names[int(cls)])
        for *xyxy, conf, cls in pred[0]
    ] if len(pred[0]) > 0 else []

    tracks = tracker.update_tracks(detections, frame=frame)

    for track in tracks:
        if not track.is_confirmed() or track.time_since_update > 0:
            continue

        track_id = track.track_id
        bbox = track.to_ltrb()
        center = (int((bbox[0] + bbox[2]) / 2), int((bbox[1] + bbox[3]) / 2))

        if track_id not in object_paths:
            object_paths[track_id] = []
        object_paths[track_id].append(center)

        analyze_crossing(track_id, object_paths[track_id])

        color = generate_unique_color(track_id)
        draw_object_path(path_frame, object_paths[track_id], color)
        annotator.box_label(bbox, f'ID: {track_id}', color=color)

    return annotator.result()

def draw_object_path(frame, path, color):
    """Draw the tracked object's path on the frame."""
    for i in range(1, len(path)):
        if path[i - 1] and path[i]:
            cv2.line(frame, path[i - 1], path[i], color, 2)

### 7. Main Run Function
def run(weights='yolov9-c.pt', device=0):
    """Run the YOLO model with manual zone definition and tracking."""
    model, names = load_model(weights, device)
    tracker = DeepSort(max_age=30, n_init=3, nn_budget=None, embedder_gpu=True, half=True)

    cap = cv2.VideoCapture("G:/UTS/2024/Spring_2024/Image Processing/Assignment/Video-Analytics-/data_/traffic_1.mp4")

    if not cap.isOpened():
        print("Error: Could not open video.")
        return

    ret, frame = cap.read()
    frame = cv2.resize(frame, None, fx=0.4, fy=0.4, interpolation=cv2.INTER_LINEAR)

    frame_height, frame_width = frame.shape[:2]

    if not ret:
        print("Error: Could not read video frame.")
        return

    cv2.namedWindow('Define Zones')
    cv2.setMouseCallback('Define Zones', mouse_callback)

    while True:
        frame_copy = frame.copy()
        draw_zones(frame_copy)
        cv2.imshow('Define Zones', frame_copy)

        if cv2.waitKey(1) & 0xFF == 13:  # Enter key to start tracking
            break

    path_frame = np.zeros_like(frame)

    while True:
        ret, frame = cap.read()
        frame = cv2.resize(frame, None, fx=0.4, fy=0.4, interpolation=cv2.INTER_LINEAR)

        if not ret:
            print("End of video.")
            break

        annotated_frame = inference(frame, model, names, tracker, path_frame)
        draw_zones(path_frame)

        path_frame[0:60, 0:400] = 0  # Clear text area

        y_offset = 0
        for i, counts in zone_counts.items():
            cv2.putText(path_frame, f'Zone {i + 1} Entered: {counts["entered"]} Exited: {counts["exited"]}',
                        (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            y_offset += 30

        cv2.imshow('YOLOv9 Detection', annotated_frame)
        cv2.imshow('Zone Analysis', path_frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    run()
